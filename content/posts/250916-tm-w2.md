+++
date = '2025-09-16'
draft = false
title = 'Text Mining | 02: Preprocessing | 预处理'
author = 'RR'
tags = ['文本挖掘', '课程笔记']
categories = ['Text Mining 课程笔记']
+++

Raw text → 

- 通过 **OCR（Optical Character Recognition）**：将原始文本，比如手写文本，经过扫描，生成数字文件（digitized documents）
- 也有一些本来就是数字形式的原始文本，比如：HTML、text 文件、PDF、MS Word
- 所有的文本都需要一些清理（clean-up）
    - 图片、表格等；设计的格式、排版；声明、版权说明；headers、footers；分列、页面空白；
    - 以及：OCR errors；character encoding errors
        - **character encoding：计算机如何将文本呈现成人类理解的方式**
    - semi-structured text：markup（HTML、XML、json）
        - markup：文本文件中的元数据（meta-information），和文本内容能区分出来。

## Character encoding

### ASCII

二级制序列 → 字符（character）

ASCII (American Standard Code for Information Interchange) 是 7 位的编码方式，基于英文字母。

### Unicode

Unicode 是所有语言系统、全球通用（universal）的标准。

不依赖于任何平台、软件、供应商（vendor）。

通过软件（app、editor、web browser）中的实现（例如 **UTF-8**）来将编码解析成字符。而软件来决定实际的呈现（大小、形状、字体、风格）。

```python
# Read and write UTF-8 in Python 3
with open(filename, 'r', encoding='utf-8') as raw:
	text = raw.read()

with open(filename, 'w', encoding='utf-8') as clean:
	clean.write(text)
```

{{<link "https://docs.python.org/3/library/functions.html#open">}}

### 关于实际中的 Data cleaning

Digitalizaiton, data conversion & cleaning：是第一步。很有必要、耗时间、会出错、常常复杂。

数据收集与清理的过程一般比较漫长。

## Regular expressions

正则表达式：removing noise, privacy sensitive information, markup。

用来捕捉、计数一个文件或字符串里面符合的模式；或者提取符合的模式。

- `Spacy`

## Tokenization & sentence splitting

### 定义

- Token：word or punctuation（单词或标点符号）
- Term：用作特征的 token，一般是 normalized form（小写）
- Token count：包含重复
- Vocabulary size |V|：不重复的 term 数。把单词用作 feature 时即为 feature size。

{{<admonition example "Example">}}

> **This issue is** **not just local — it** is **a global** **problem.**
> 

Token count: 13

Unique terms: 10（不包括标点和重复的单词）

Vocabulary size |V|: 10（同上）

{{< /admonition >}}

### Tokenization

将文本分割成 tokens。

1. 去掉标点
2. 按空格分割文本

### Subword tokenizers

不再把 tokens 定义成按空格分开的单词或是中文的汉字，而是根据数据，能自动判断哪些是 tokens。

- 尤其适合处理不认识的新词
    - 回顾 Heap's Law：在某集合上训练模型时，新添加的词会越来越多
- 解决方案： sets of tokens → subwords
    - 把低频词分成词汇表里存在的 tokens。
    - subwords 本身可能并不是单词
        - proportionate → prop  portion  ate
- Byte-pair encoding（BPE）
- WordPiece/SentenePiece

#### Byte-pair encoding（BPE）

1. begin with a vocabulary
2. 检查要训练的 corpus，选择相邻着出现的频率最高的两个符号
3. 把这两个符号连起来（例如：t、h → th）添加进 vocabulary
4. 把所有相连着出现的这两个符号，都替换成连起来的新字符串（例如‘th‘）
5. 重复第二步，创造新的越来越长的字符串
6. 直到完成了 k 次，创建了 k 个新的 tokens（k 是参数）

最后，vocabulary 里：原有的字符 + k 个新 tokens

#### WordPiece/SentencePiece

基于 BPE，高效的实现。

vocabulary size 固定，优化 tokens 的选择。

高频出现的词语成为 token，低频词被分割成 subwords（例如：Gorlaeus → Go + ##rl + ##ae + ##us’）。

## Lemmatization & Stemming

- Lemma（词形）
- Stem（词干）
- 减少 word vector space 中的 features 总数（就是减少单词数量的意思）
- 更好地泛化，尤其是小数据集

### Lemma 词形

定义：dictionary form of a word

{{<admonition example "举例：">}}

- 动词的原形（infinitive of verbs）
    - think ← thinks, thinking, thought
- 名词的单数形式（singular form of nouns）
    - mouse ← mice
    - computer ← computers

{{< /admonition >}}

Lemmatization 需要词典。

### Stem 词干

定义：the portion of a word that is common to a group of words with the same prefix (base/root)

相关的单词会有相同的词干。

{{<admonition example "举例：">}}

computer, computing, computers, compute

→ ‘comput’

{{< /admonition >}}

词干自己不一定是一个存在的单词。

在长得不一样的单词中，词干总是那个一直不变的形式。就比如前一个例子里的那个“comput”。

---

我们一般总是更倾向于 lemma 而不是 stem。
非常小的集合，或是资源很少的语言，stemming 的效果才会更好。

Note：
因为 lemmazation 基于字典，把词还原成原形，结果是真实的单词，能看懂含义。还擅长不规则变形的词形还原，比如 better → good。
Stemming 是基于规则，比较生硬，得到的结果往往都不是真实的单词，人看不懂含义。不擅长不规则变形的单词，better 可能没法还原成 good。
但是，低资源的语言（如某些土著语言、濒危语言）没有字典、没有标注数据。

{{<admonition example "用一个例子来总结">}}

Original text:

> Such an analysis can reveal features that are not easily visible from the variations in the individual genes and can lead to a picture of expression that is more biologically transparent and accessible to interpretation
> 
1. Lematizer: 
Such an analysis can reveal feature that **be** not easily visible from the variation in the individual **gene** and can lead to a picture of expression that **be** more biologically transparent and accessible to interpretation
2. Stemmer:
Such an **analysi** can reveal **featur** that **ar** not **easili visibl** from the **variat** in the **individu gene** and can lead to a **pictur** of **express** that is more **biolog transpar** and **access** to **interpret**
3. Subword tokenizer:
Such an analysis can reveal features that are not easily visible from the variations in the individual genes and can lead to a picture of expression that is more biological **##ly** transparent and accessible to interpretation

{{< /admonition >}}

## Edit distance

衡量字符串相似度时，如何处理拼写错误、同一个词不同的拼写方式呢？

最小编辑距离（minimum edit distance）：把一个字符串变成另一个字符串所需要的最小编辑操作次数

- cost of 1：
    - **insertion**
        - cat → coat
    - **deletion**
        - cat → at
    - **substitution**（替换）
        - cat → cut
- *free：copying charaters*

把一个单词变成另一个单词，有很多路径，但总有一条路最简单（cheapest）

比如：sheep → ship
- copy ‘sh’, delete ‘e’, substitute ‘e’ by ‘p’ (edit distance = 2) 
- copy ‘sh’, delete ‘e’, delete ‘e’, insert’i’, copy ‘p’ (edit distance = 3)

- 计算最小编辑距离：Dynamic programming。是一类算法，用一种基于表格的方法来解决问题。

### Levenshtein distance algorithm

- 画一个表格。左边第一列是原本的单词，上边第一行是目标单词。
    - 往右走：Insert
    - 往下走：Delete
    - 走对角线：Substitute or Copy
    - 选择最小消耗的行动
    - 直到走到右下角

消耗依然是：Insert(1), Delete(1), Substitute(1), Copy(0)

{{<image "/images/course_note/tm-02-1.png">}}

{{<image "/images/course_note/tm-02-2.png">}}

{{<image src="/images/course_note/tm-02-3.png" caption="图片来源：Leiden University Text Mining Course Slides">}}

