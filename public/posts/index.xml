<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>所有文章 - RR的博客</title>
        <link>http://example.org/posts/</link>
        <description>所有文章 | RR的博客</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Mon, 27 Oct 2025 00:00:00 &#43;0000</lastBuildDate><atom:link href="http://example.org/posts/" rel="self" type="application/rss+xml" /><item>
    <title>模型量化技术</title>
    <link>http://example.org/251027-quantization/</link>
    <pubDate>Mon, 27 Oct 2025 00:00:00 &#43;0000</pubDate>
    <author>RR</author>
    <guid>http://example.org/251027-quantization/</guid>
    <description><![CDATA[<em>A Visual Guide to Quantization</em> 模型量化技术笔记]]></description>
</item>
<item>
    <title>Vit 模型：《An Image is Worth 16x16 Words: Transformers for Image Recognitoin at Scale》论文笔记</title>
    <link>http://example.org/251026-vit/</link>
    <pubDate>Sun, 26 Oct 2025 00:00:00 &#43;0000</pubDate>
    <author>RR</author>
    <guid>http://example.org/251026-vit/</guid>
    <description><![CDATA[Vit 模型：《An Image is Worth 16x16 Words: Transformers for Image Recognitoin at Scale》论文笔记]]></description>
</item>
<item>
    <title>Transformers 模型：《Attention is All You Need》论文笔记</title>
    <link>http://example.org/251024-transformers/</link>
    <pubDate>Fri, 24 Oct 2025 00:00:00 &#43;0000</pubDate>
    <author>RR</author>
    <guid>http://example.org/251024-transformers/</guid>
    <description><![CDATA[Transformers 模型：《Attention is All You Need》论文笔记]]></description>
</item>
<item>
    <title>Text Mining | 03: Vector Semantics | 语义向量与词嵌入</title>
    <link>http://example.org/250923-tm-w3/</link>
    <pubDate>Tue, 23 Sep 2025 00:00:00 &#43;0000</pubDate>
    <author>RR</author>
    <guid>http://example.org/250923-tm-w3/</guid>
    <description><![CDATA[关于语义向量、word2vec，以及词嵌入（word embedding）的深入理解。]]></description>
</item>
<item>
    <title>Text Mining | 02: Preprocessing | 预处理</title>
    <link>http://example.org/250916-tm-w2/</link>
    <pubDate>Tue, 16 Sep 2025 00:00:00 &#43;0000</pubDate>
    <author>RR</author>
    <guid>http://example.org/250916-tm-w2/</guid>
    <description><![CDATA[<p>Raw text →</p>
<ul>
<li>通过 <strong>OCR（Optical Character Recognition）</strong>：将原始文本，比如手写文本，经过扫描，生成数字文件（digitized documents）</li>
<li>也有一些本来就是数字形式的原始文本，比如：HTML、text 文件、PDF、MS Word</li>
<li>所有的文本都需要一些清理（clean-up）
<ul>
<li>图片、表格等；设计的格式、排版；声明、版权说明；headers、footers；分列、页面空白；</li>
<li>以及：OCR errors；character encoding errors
<ul>
<li><strong>character encoding：计算机如何将文本呈现成人类理解的方式</strong></li>
</ul>
</li>
<li>semi-structured text：markup（HTML、XML、json）
<ul>
<li>markup：文本文件中的元数据（meta-information），和文本内容能区分出来。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="character-encoding">Character encoding</h2>
<h3 id="ascii">ASCII</h3>
<p>二级制序列 → 字符（character）</p>]]></description>
</item>
<item>
    <title>Text Mining | 01: Introduction</title>
    <link>http://example.org/250907-tm-w1/</link>
    <pubDate>Sun, 07 Sep 2025 00:00:00 &#43;0000</pubDate>
    <author>RR</author>
    <guid>http://example.org/250907-tm-w1/</guid>
    <description><![CDATA[<p>Text mining: Automatic extraction of knowledge from text
从文本中自动地提取知识（信息）</p>
<p>Text: unstructured; Knowledge: structured</p>
<p>Big text data —(Text retrievl)→ Small relevant data —(Text mining)→ Knowledge → Many applications</p>
<p><strong>Challenges of text data:</strong></p>]]></description>
</item>
</channel>
</rss>
