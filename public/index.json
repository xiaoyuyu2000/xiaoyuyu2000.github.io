[{"categories":["大模型技术"],"content":" 参考链接 原文：Maarten Grootendorst: A Visual Guide to Quantization 中文翻译版：模型量化技术可视化指南 ","date":"2025-10-27","objectID":"/251027-quantization/:0:0","tags":["模型量化"],"title":"模型量化技术","uri":"/251027-quantization/"},{"categories":["大模型技术"],"content":"1. LLMs存在的问题 大语言模型（LLM）的参数数量能达到几十亿，其中包括权重参数（weights）、激活值（activations）等。我们的目标：用尽量高效的方式表达这巨量的参数，以减少存储参数所需要的空间。 ","date":"2025-10-27","objectID":"/251027-quantization/:1:0","tags":["模型量化"],"title":"模型量化技术","uri":"/251027-quantization/"},{"categories":["大模型技术"],"content":"参数数值（value）的表示方法 数值通常以浮点数形式来表示，带有正负号和小数点。 这些数值由 bits 组成，也就是二进制数字。这些 bits 可以被分成三个部分： 符号位（Sign） 指数部分（Exponent） 小数部分（也称为尾数）（Significand / Mantissa） 符号位、指数部分、小数部分结合在一起，就能根据一组特定的 bits 来计算出一个具体的数值了。 FP16 1 个符号位 + 5 个指数位 + 9 个小数位 举个例子： 某个FP16（16位浮点数），其二进制表示为：0 10000 1001001000 符号位：0，代表符号位是 $(-1)^0=1$，符号为正 指数部分：10000，代表 $2^{2^4-(2^{(5-1)}-1)=1}=2^1=2$ 小数部分：1001001000，代表 $2^{-1}+2^{-4}+2^{-7}=0.5703125$ 所以这个浮点数实际表示为：$1\\times 2\\times 1.5703125=3.140625$ ","date":"2025-10-27","objectID":"/251027-quantization/:1:1","tags":["模型量化"],"title":"模型量化技术","uri":"/251027-quantization/"},{"categories":["大模型技术"],"content":"内存限制问题 用来表示数值的位数（bit）越多： 得到的数值精确度越高 能表示的数值范围越大 占用空间也越大 动态范围（dynamic range）：指的是某个数值表示法所能表示的所有数值的区间。 精度（precision）：相邻两个数值之间的间隔。（相邻两数值离得越近，说明这个数值表示法的精度越高） 我们可以计算出存储一个特定数值所需要的设备内存量。一个字节（byte）等于8位（bits），所以可以位大多数浮点表示形式制定一个基本的计算公式： $memory=\\frac{nr_bits}{8}\\times nr_params$ nr_bits：数值表示法用来表示数值的比特位数 nr_params：模型中的参数数量 实际应用中，模型推理阶段所需的显存（VRAM）量还受别的因素影响（比如模型处理上下文的大小、模型架构设计） 举例说明：假设有一个 700 亿（70B）参数的模型 使用 FP32（32 位浮点数，常称为全精度）数值表示法： $memory=\\frac{32}{8}\\times70B=280 GB$ 使用 FP64：64/8 * 70B = 560GB 使用 FP16：16/8 * 70B = 140GB 由此可见，有必要尽可能减少用于表示模型参数数值的 bits 数量。然而，bits 数减少，精度会降低，从而导致力模型准确性下降。 目标：减少用于表示模型参数的 bits 数量，同时又不损害模型的准确性。 ——这就是模型量化技术的目的。 ","date":"2025-10-27","objectID":"/251027-quantization/:1:2","tags":["模型量化"],"title":"模型量化技术","uri":"/251027-quantization/"},{"categories":["大模型技术"],"content":"2. 模型量化技术简介 模型量化的核心：将模型参数的精度从较高的位宽（bit-widths）（比如32位浮点数）降低到较低的位宽（例如8位整数）。 减少比特位数时，会出现精度损失。 模型量化的主要目的就是减少表示原始参数所需的 bits 数量，同时尽可能保留原始参数的精度。（上文已经说过） ","date":"2025-10-27","objectID":"/251027-quantization/:2:0","tags":["模型量化"],"title":"模型量化技术","uri":"/251027-quantization/"},{"categories":["大模型技术"],"content":"常用的数据类型 除了32-bit全精度（full-precisoin，FP32）之外，还有多种不同的数据类型。 FP16 16-bit 浮点数，为半精度浮点数，也称为 FP16。 举例说明，将 32-bit 转换成 16-bit（半精度，FP16）浮点数： 某个 FP16（16 位浮点数），其二进制表示为：0 10000 1001001000 FP16 的数值范围比 FP32 窄得多，且数值有一定的精度损失（3.14159274… → 3.140625） ","date":"2025-10-27","objectID":"/251027-quantization/:2:1","tags":["模型量化"],"title":"模型量化技术","uri":"/251027-quantization/"},{"categories":["大模型技术"],"content":"BF16 为了保持与原始 FP32 相似的数值范围，引入了 bfloat 16这一数据类型（即 BF16，brain-float 16），相当于“截断版的 FP32”。 FP32 与 BF16： 某个 FP16（16 位浮点数），其二进制表示为：0 10000 1001001000 表示的数值范围是相同的，但 BF16 只有 16 bits。BF16 在深度学习领域得到了广泛应用。 INT8 进一步减少 bits 的数量，就要使用整数表示法了，比如仅有 8 bits 的 INT8。INT8 占用的 bits 数量仅仅为 FP32 全精度浮点数的四分之一。 FP32 与 INT8： 某个 FP16（16 位浮点数），其二进制表示为：0 10000 1001001000 FP32 的数值范围： $[-3.4e^{38},~3.4e^{38}]$ INT8 的数值范围： $[-127,~128]$ 实际应用中，并不需要将 FP32 所表示的全部数值范围都映射到 INT8，我们只需要找到一种方法，将数据（即实际的模型的参数）范围映射到 INT8 即可。 常用的压缩（squeezing）和映射（mapping）方法包括对称量化、非对称量化，它们都是线性映射（linear mapping）的不同形式。 接下来讨论将 FP32 量化成 INT8 的方法。 ","date":"2025-10-27","objectID":"/251027-quantization/:2:2","tags":["模型量化"],"title":"模型量化技术","uri":"/251027-quantization/"},{"categories":["大模型技术"],"content":"对称量化（Symmetric Quantization） 对称量化中，原本浮点数的值域会被映射到量化空间（quantized space）中的一个以零为中心的对称区间。量化前后的值域都是围绕零点对称的。 在 FP32 中表示零的值，在 INT8 中依然正好表示零。 对称量化的一种经典方法：绝对最大值（absmax，absolute maximuum）量化。 量化的过程 从一组数据中找出最大的绝对值 α，以此作为线性映射的范围（从 -α 到 +α）。 计算比例因子（s，scale factor）： $s=\\frac{2^{b-1}-1}{\\alpha}$ b 是我们想要的量化结果的 bits 数（这里 INT8 为 8）。 α 是最大的绝对值 计算量化结果： $X_{quantized}=round(s\\cdot X)$ 举例说明： $X=[5.47,~3.08,~-7.59,~0,~-1.95~,-4.57,~10.8]$ α = 10.8，所以线性映射范围设为 -10.8 ~ 10.8 比例因子： $s=\\frac{2^{8-1}-1}{10.8}=\\frac{127}{10.8}\\approx 11.76$ 量化： $X_{quantized}=round(11.76\\cdot X)$ $=round([64.3272, ~36.2208, ~-89.2584, ~0, ~-22.932, ~-53.7432, ~127.008])$ $=[64, ~36, ~-89, ~0, ~-23, ~-54, ~127]$ 反量化的过程 把量化后的 INT8 结果恢复成原始的 FP32 数值：使用之前计算出的比例因子 s ，对量化后的数值进行反量化（dequantize）。 量化误差（quantization error）：指的是原始值经过量化，然后再反量化回到原始数值表示法这个过程中，原始值（original values）与反量化值（dequantized values）之间的差值。 反量化的计算： $X_{dequantized}=\\frac{X_{quantized}}{s}$ 量化后的结果，除以比例因子 s，即可 继续以前边量化的例子来解释反量化： $X_{dequantized}=\\frac{[64, ~36, ~-89, ~0, ~-23, ~-54, ~127]}{11.76}\\approx[5.44, ~3.06, ~-7.54, ~0, ~, -1.96, ~-4.59, ~10.8, ~3.06, ~-1.96]$ 整个量化与反量化的过程，如图： 观察到，某些值量化到 INT8 之后，被分配到了相同的值（36），而反量化回到 FP32 时，它们也会变成相同的浮点数值。 量化误差：（比较原始值与反量化值） 一般来说，量化后的 bits 数越小，量化误差往往越大。 ","date":"2025-10-27","objectID":"/251027-quantization/:2:3","tags":["模型量化"],"title":"模型量化技术","uri":"/251027-quantization/"},{"categories":["大模型技术"],"content":"非对称量化（Asymmetric Quantization） 非对称量化并不是以零为中心对称的，它将浮点数范围中的最小值 β 和最大值 α 映射到量化范围（quantized range）的最小值和最大值。 我们在此讨论的方法称为零点量化。 量化的过程 找出数据中的最大值与最小值，以此来作为线性映射的范围。 计算比例因子（s，scale factor）： $s=\\frac{2^{b-1}-(-(2^{b-1}-1))}{\\alpha-\\beta}=\\frac{128-(-127)}{\\alpha-\\beta}=\\frac{255}{\\alpha-\\beta}$ 计算零点（zeropoint）： $z=round(-s\\cdot \\beta)-2^{b-1}$ 计算量化结果： $X_{quantized}=round(s\\cdot X+z)$ 举个例子来解释非对称量化： 线性映射的范围为： $[-7.59,~10.8]$ 最小值与最大值到零点的距离是不相等的——“非对称” 计算比例因子 s： $s=\\frac{255}{10.8-(-7.59)}\\approx13.86$ 计算零点 z： $z=round(-13.86\\cdot (-7.59))-2^7=-23$ 即：FP32中的数值 0，在 INT8 中被映射到了 -23的位置。 计算量化结果： $X_{quantized}=round(13.86\\cdot […]+z)$ $=[53, ~20, ~-128, ~-23, ~-50, ~-86, ~127]$ 反量化的过程 反量化的计算： $X_{dequantized}=\\frac{X_{quantized}-z}{s}$ ","date":"2025-10-27","objectID":"/251027-quantization/:2:4","tags":["模型量化"],"title":"模型量化技术","uri":"/251027-quantization/"},{"categories":["大模型技术"],"content":"取值范围的映射与剪裁 将向量中的数值映射到更低的位表示形式，前边介绍的方法使得向量的整个范围都能被映射，但一个明显的缺点：有离群值（outlier）的时候，不太好处理。 离群值（outlier）：向量中的一个数值远大于（或远小于）其他所有数值，该数值就可以被视为离群值。 举个例子，来说明有离群值的时候，对数值的映射的危害： $X=[-0.32,~0.89,~0.45,~256]$ → 离群值：256 用非对称量化： 计算比例因子 s： $s=\\frac{255}{256-(-0.32)}\\approx0.99$ 计算零点 z： $z=round(-0.99\\cdot(-0.32))-2^7=-128$ 计算量化结果： $X_{quantized}=round(0.99\\cdot[-0.32, ~0.89, ~0.45, ~256])+z$ $=[0, ~1, ~0, ~253]+(-128)$ $[-128, ~-127, ~-128, ~125]$ 所有较小的数值，都被映射到了相近的 INT8 数值，并因此失去了它们的独特特性。（为了“照顾到”离群值 256……） 我们需要进行剪裁（clipping）。 一种简单的方法是：为原始值设定一个不同的动态范围，而所有离群值都会被映射到相同的值。 比如：假如我们将动态范围设置为 $[-5,~5]$，那所有超出这个范围的数值，都会被当做 -5 或 5 来看待，于是被映射成 -128 或 127。 优点：减少了非离群值的量化误差 缺点：增加了离群值的量化误差 ","date":"2025-10-27","objectID":"/251027-quantization/:2:5","tags":["模型量化"],"title":"模型量化技术","uri":"/251027-quantization/"},{"categories":["大模型技术"],"content":"校准过程（Calibration） 前文所说的手动选择一个动态范围，这个过程被称为校准（calibration）。 目的：找到一个能够包含尽可能多的数值（values）的范围，同时尽量减少量化误差。 不同的参数，校准方法不同。 权重（和偏置项）Weights (and Biases) LLMs中，可以将权重（weights）和偏置项（biases）视为预先确定的静态值，因为这些值在模型运行之前就确定了。偏置项的数量远小于权重的数量，偏置项通常被保留在更高的精度（比如 INT16），而量化的主要工作集中在权重的处理。 手动选择输入范围的百分位数 会导致与前文提过的相似的裁剪（clipping）行为 优化原始权重和量化权重之间的均方误差（MSE） 最小化原始值和量化值之间的熵（KL 散度） 激活值 LLMs 中，在整个推理过程中，持续更新的输入（input）通常被称为激活值（activations）。这些激活值往往需要通过激活函数进行处理，比如 sigmoid、 ReLU。 与权重不同，激活值会随着每次输入数据的改变而改变，因此很难进行精确量化。 校准权重和激活值的量化方法主要有两种： Post-Training Quantization（PTQ）：训练完成后进行量化 Quantization Aware Training（QAT）：训练/微调过程中同时进行量化 ","date":"2025-10-27","objectID":"/251027-quantization/:2:6","tags":["模型量化"],"title":"模型量化技术","uri":"/251027-quantization/"},{"categories":["大模型技术"],"content":"3. 训练后量化（Post-Training Quantization，PTQ） Post-Training Quantization（PTQ）是在训练完模型之后，再对模型的参数（包含权重、激活值）进行量化。 对于权重值的量化：（主要两种） 对称量化（symmetric quantization） 非对称量化（asymmetric quantization） 对于激活值的量化：因为我们不知道激活值的范围，所以需要通过模型的推理，来获取它们的 potential distribution（也就是在不同的输入与模型参数下，激活值可能出现的范围分布，然后我们根据这个分布来选择一个能包含大部分激活值的量化级别），然后再进行量化。（主要两种） 动态量化（dynamic quantization） 静态量化（static quantization） ","date":"2025-10-27","objectID":"/251027-quantization/:3:0","tags":["模型量化"],"title":"模型量化技术","uri":"/251027-quantization/"},{"categories":["大模型技术"],"content":"动态量化（Dynamic Quantization） 当数据通过隐藏层时，其激活值会被收集起来。 随后，利用这些激活值的分布（distribution of activations），来计算量化输出值所需的零点（z）和比例因子（s） $s=\\frac{255}{\\alpha-\\beta}$ $z=round(-s\\cdot \\beta)-128$ $X_{quantized}=round(s\\cdot X+z)$ 每一层都重复这个过程。每一层都有其独特的零点 z 和 比例因子 s，每一层的量化方案不同。 ","date":"2025-10-27","objectID":"/251027-quantization/:3:1","tags":["模型量化"],"title":"模型量化技术","uri":"/251027-quantization/"},{"categories":["大模型技术"],"content":"静态量化（Static Quantization） 静态量化在模型推理过程中，并不实时地计算零点（z）和比例因子（s），而是在模型训练或校准的过程中提前计算。 为了找到这些值，会使用一个校准数据集，并让模型处理这些数据，来收集可能的激活值分布（potential distribution）。 收集到这些数值后，就可以计算出必要的 s 和 z，以便于在推理过程中量化。 实际推理过程中，s 和 z 不需要重新计算，而是被应用于所有激活值，实现全局量化。 动态量化一般来说精度会更高，因为为每一个隐藏层都计算一次 s 和 z 值，但因此也会增加计算时间。 而静态量化虽然准确度略低，但由于已知了用于量化的 s 和 z，因此推理时更为高效。 ","date":"2025-10-27","objectID":"/251027-quantization/:3:2","tags":["模型量化"],"title":"模型量化技术","uri":"/251027-quantization/"},{"categories":["大模型技术"],"content":"探索 4-bit 量化的极限 两种在 HuggingFace 上常用的将量化位数进一步降到 6-bit、4-bit甚至 2-bit（但不建议低于 4-bit）的方法： GPTQ：全模型在 GPU 上运行 GGUP：将一部分模型层从 GPU 转移到 CPU 上执行 GPTQ 采用非对称量化，逐层处理，每层都经过独立处理。 首先将模型的权重转换为 Hessian 矩阵（二阶偏导数矩阵，用于描述函数在其输入变量上的局部曲率。对于多变量函数，Hessian 矩阵可以用来了解函数在某点上的凹凸性、函数值对输入变量的变化有多敏感）的逆矩阵，它是模型损失函数的二阶导数，告诉我们模型输出对每个权重变化的敏感程度。 该过程展示了模型层中每个权重的重要性（权重的影响程度）。 与 Hessian 矩阵中较小值相关的全助攻更重要，因为它们的微小变化可能对模型性能产生重大影响。 GGUF ","date":"2025-10-27","objectID":"/251027-quantization/:3:3","tags":["模型量化"],"title":"模型量化技术","uri":"/251027-quantization/"},{"categories":["大模型技术"],"content":"4. 训练中量化（Quantization Aware Training，QAT） QAT 的目标是在训练过程中学习量化过程。 QAT 通常比 PTQ 更准确。 在训练过程中，引入所谓的“伪”量化，比如先将权重量化到例如 INT4 等形式，然后将它们再反量化回 FP32。 这一过程使得模型在训练阶段进行损失值计算和权重更新时，能够考虑到量化误差。 QAT 试着探索损失函数中的“宽”最小值区域（“wide” minima），以尽可能减少量化误差，因为“窄”最小值区域（“narrow” minima）往往会导致更大的量化误差。 ","date":"2025-10-27","objectID":"/251027-quantization/:4:0","tags":["模型量化"],"title":"模型量化技术","uri":"/251027-quantization/"},{"categories":["论文阅读"],"content":" 原始论文 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2021): [PDF] 一张图片就相当于 16x16 的（若干）单词——用于大规模图形识别的 Transformer ","date":"2025-10-26","objectID":"/251026-vit/:0:0","tags":["Transformer","论文阅读","CV"],"title":"Vit 模型：《An Image is Worth 16x16 Words: Transformers for Image Recognitoin at Scale》论文笔记","uri":"/251026-vit/"},{"categories":["论文阅读"],"content":"简单介绍 这篇文章主要任务，就是通过 Transformer 结构和自注意力机制，把计算机视觉的任务，当做自然语言处理的任务去完成。主要思想就是把图片分成若干个 16x16 尺寸的 patches，把每个 patch 都当成 token，然后用监督学习训练图片分类的任务。 ","date":"2025-10-26","objectID":"/251026-vit/:1:0","tags":["Transformer","论文阅读","CV"],"title":"Vit 模型：《An Image is Worth 16x16 Words: Transformers for Image Recognitoin at Scale》论文笔记","uri":"/251026-vit/"},{"categories":["论文阅读"],"content":"Motivation Transformer 适用于 NLP（natural language processing，自然语言处理）任务 而 CNN（convolutional neural network，卷积神经网络）依然是计算机视觉（computer visoin，CV）领域的主流。 于是很多工作在尝试将 Transformer 的自注意力机制用于 CV 领域。 ","date":"2025-10-26","objectID":"/251026-vit/:1:1","tags":["Transformer","论文阅读","CV"],"title":"Vit 模型：《An Image is Worth 16x16 Words: Transformers for Image Recognitoin at Scale》论文笔记","uri":"/251026-vit/"},{"categories":["论文阅读"],"content":"Vision Transformer (ViT) 的主要思想 Chapter 1 Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modifications. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. We train the model on image classification in supervised fashion. 把 Transformer 直接用于图片：首先把图片分割成 patches，然后把这些 patches的线性嵌入（linear embeddings）的序列作为 Transformer 结构的输入。 图片的 patches 就跟 NLP 任务里的词元（tokens）（也就是单词）类似：每个 patch 就相当于 NLP 里的一个单词。 我们用监督学习的方式训练模型用于图片分类的任务。 序列长度的变化：（序列长度等于 patch 的个数） 一个图片原本的尺寸：224x224=50124 个像素 把它分成若干个 16x16 尺寸的 patches 之后： 每一行和每一列的“序列数目”就变成了：224/16=14 所以，序列长度就从原先的 50124 变成了 14x14=196 （就相当于 NLP 中，一个 196 个 token 的语句） ","date":"2025-10-26","objectID":"/251026-vit/:2:0","tags":["Transformer","论文阅读","CV"],"title":"Vit 模型：《An Image is Worth 16x16 Words: Transformers for Image Recognitoin at Scale》论文笔记","uri":"/251026-vit/"},{"categories":["论文阅读"],"content":"ViT的结构 尽可能地保持原始的 Transformer 结构不更改——这是 ViT 的一项优势。 结构图如下：（来自论文 Figure 1） 图1：ViT 模型架构 (Dosovitskiy et al., 2021) ","date":"2025-10-26","objectID":"/251026-vit/:3:0","tags":["Transformer","论文阅读","CV"],"title":"Vit 模型：《An Image is Worth 16x16 Words: Transformers for Image Recognitoin at Scale》论文笔记","uri":"/251026-vit/"},{"categories":["论文阅读"],"content":"ViT结构的详细解释 首先，给定一张图，把这张图分解成了若干个 patch Linear Projection of Flattened Patches（线性投射层）： a. 把这些 patch 当成一个序列，每个 patch 都会经过一个线性投射层的操作，得到一个特征，也就是 Patch + Position Embedding 相当于 NLP 中的原始语义+位置编码 得到的结果就相当于 NLP 中的 token b. 利用 BERT 的 Extra learnable embedding 的特殊字符 cls 分类字符： 也就是结构图中的星号部分的“token”，位置编码永远是 0 我们只需要根据它的输出，作最后图片分类的判断。 Transformer Encoder：上一步得到的结果作为 Transformer 编码器的输入 MLP Head：通用的分类头 分类：交叉熵函数 ","date":"2025-10-26","objectID":"/251026-vit/:3:1","tags":["Transformer","论文阅读","CV"],"title":"Vit 模型：《An Image is Worth 16x16 Words: Transformers for Image Recognitoin at Scale》论文笔记","uri":"/251026-vit/"},{"categories":["论文阅读"],"content":"图片尺寸与序列大小 原先：224x224x3 每个 16x16x3 的 patch：16x16x3=768 个像素大小 总共分成了多少个 patch：224/16=14，14x14=196 个 patch （等于输入的序列大小） ——原先的图片被分成了 196 个 patch，每个 patch 的维度是 768 （NLP 任务中：一句话包含了若干个 token，每个 token 的维度是 512） 矩阵尺寸 输入：X → 196x768 经过线性投射层之后：197x768（加上了那个 Extra learnable embedding 的特殊字符 cls 分类字符，也就是加了 1） 多头自注意力： 总共 12 个头 768/64=12 而 NLP 中：是 512/64=8 个头 ","date":"2025-10-26","objectID":"/251026-vit/:3:2","tags":["Transformer","论文阅读","CV"],"title":"Vit 模型：《An Image is Worth 16x16 Words: Transformers for Image Recognitoin at Scale》论文笔记","uri":"/251026-vit/"},{"categories":["论文阅读"],"content":" 参考链接 Attention is All You Need (Vaswani et al., 2017)：[PDF] 一位B站博主的视频讲解：《Attention is all you need》论文解读及Transformer架构详细介绍 序列转导模型（sequence transduction models）：比如翻译任务，输入内容的前后顺序很重要。 Transformer之前的模型： RNN（recurrent neural network，循环神经网络）和CNN（convolutional neural network，卷积神经网络） 包括编码器（encoder）和解码器（decoder） 使用注意力机制（attention mechanism）增强 Transformer结构的创新： 完全摒弃了RNN和CNN（但仍然使用encoder-decoder结构） 完全基于注意力机制 ","date":"2025-10-24","objectID":"/251024-transformers/:0:0","tags":["Transformer","论文阅读","LLM","NLP"],"title":"Transformers 模型：《Attention is All You Need》论文笔记","uri":"/251024-transformers/"},{"categories":["论文阅读"],"content":"背景知识 ","date":"2025-10-24","objectID":"/251024-transformers/:1:0","tags":["Transformer","论文阅读","LLM","NLP"],"title":"Transformers 模型：《Attention is All You Need》论文笔记","uri":"/251024-transformers/"},{"categories":["论文阅读"],"content":"Feedforward Neural Network (FNN) 前馈神经网络 不适合序列转导任务。 序列转导任务：分词（Tokenization）→ 词向量表示（Embedding）：把词元用一组N维向量来表示，例如 $[0.2,0.4,-0.4,0.5]$ 这种 → 合并词向量（平均 or 拼接） 平均：完全丢失了词语的顺序 拼接：FNN需要固定维度的输入，对不同长度的句子处理效率低下；FNN将句子作为一个整体来处理，无法理解真正的先后顺序的关系。 ","date":"2025-10-24","objectID":"/251024-transformers/:1:1","tags":["Transformer","论文阅读","LLM","NLP"],"title":"Transformers 模型：《Attention is All You Need》论文笔记","uri":"/251024-transformers/"},{"categories":["论文阅读"],"content":"Recurrent Neural Network (RNN) 循环神经网络 RNN能解决的问题： 构建词序：RNN按时间顺序（token顺序）逐个处理输入 构建上下文依赖：RNN逐个地喂入词语，并有“记忆”机制 支持不定长输入：不再需要FNN那种固定长度的输入，句子多长都可以。 RNN 模拟人说话的过程示例 RNN像是在模拟人说话的过程，举个例子说明。（“我爱小猫”） $$h_t=g(Wx_t+Uh_{t-1})$$ $$y_t=g(Vh_t)$$ t1时刻： 输入第一个 token “我” $x_1$ 与矩阵权重 $W$ 相乘，再加上一个初始化的隐藏状态 $h_0$ 乘上权重矩阵 $U$ 相加的结果输入给激活函数 $g(x)$，得到一个 $h_1$（$h_1$ 是用于下一时刻的输出） $h_1$ 与另一个权重矩阵 $V$ 相乘 相乘结果也经过一个激活函数 $g(x)$，得到 $y_1$（$y_1$ 是 $t_1$ 时刻的实际输出） ","date":"2025-10-24","objectID":"/251024-transformers/:1:2","tags":["Transformer","论文阅读","LLM","NLP"],"title":"Transformers 模型：《Attention is All You Need》论文笔记","uri":"/251024-transformers/"},{"categories":["论文阅读"],"content":"Encoder and Decoder 编码器 - 解码器结构 编码器处理输入，得到一个最终的编码结果；解码器使用这个编码结果，去给出最终的输出。 编码器：其实就是一个RNN神经网络去掉了输出 $y$ 的部分，只保留输出 $h$ 的部分。 编码器将得到一个上下文向量 $C$（Context vector） 上下文向量是对整个输入序列的语义编码，是一个固定长度的向量，涵盖了整个输入文本的语义信息。 $C$ 等于最后一个时间步的隐藏状态输出 $h_t$ $C$ 作为 Decoder 的输入，用于生成目标序列。 解码器的解码方式： 最简单的方式：第一个时刻，隐藏状态的输入 $s_0$，得到第一个时间步的输出结果（也就是第一个 token 的解码结果 “I”），这个输出结果会送入第二个时间步成为它的输入，而第一个时间步还得到一个隐藏状态的输出结果 $s_1$，$s_1$ 也会送入第二个时间步成为它的输入。进行下去直到得到所有时间步的输出结果。 另一种方式：由于可能解码到后边的时候，上下文信息已经被稀释了很多，可以给每个时间步重新输入一次上下文向量 $C$。 Encode-Decode的问题 处理长序列时，有“遗忘”的问题：远距离依赖信息在传递过程中，会被稀释。 不同时间步的输入对当前时刻输出的“重要性”问题：所有时间步的输入，在计算当前时刻输出时，被等同对待，忽略了不同时间步对当前时刻输出的重要性可能存在差异。 ","date":"2025-10-24","objectID":"/251024-transformers/:1:3","tags":["Transformer","论文阅读","LLM","NLP"],"title":"Transformers 模型：《Attention is All You Need》论文笔记","uri":"/251024-transformers/"},{"categories":["论文阅读"],"content":"Attention Mechanism 注意力机制 在解码器的部分，给每个时间步重新输入上下文向量C的时候：给每一个时间步输入的上下文向量C的值不一样。 $$C_i=\\sum_{j=1}^n\\alpha_{ij}h_j$$ 举例说明 比如之前那个例子：“我爱小猫” → “I love little cats” $C_0=0.6h_1+0.1h_2+0.2h_3+0.1h_4$ $C_1=0.2h_1+0.7h_2+0.1h_3+0h_4$ $C_2=0.1h_1+0.1h_2+0.4h_3+0.4h_4$ → 更“注意”一下“小”旁边的“猫”字 $C_3=。。。$ 依然存在的问题：串行化计算 递归计算、顺序计算——总要在前一个时间步结束后，才能进行下一个时间步的计算，导致训练过程无法并行化。 Transformer结构 摒弃了RNN，为了解决顺序计算问题 也不使用CNN（CNN可以解决顺序计算的问题，但却又引入了RNN已经解决掉了的远距离依赖随着序列变长而被稀释的问题） ","date":"2025-10-24","objectID":"/251024-transformers/:1:4","tags":["Transformer","论文阅读","LLM","NLP"],"title":"Transformers 模型：《Attention is All You Need》论文笔记","uri":"/251024-transformers/"},{"categories":["论文阅读"],"content":"Transformer ","date":"2025-10-24","objectID":"/251024-transformers/:2:0","tags":["Transformer","论文阅读","LLM","NLP"],"title":"Transformers 模型：《Attention is All You Need》论文笔记","uri":"/251024-transformers/"},{"categories":["论文阅读"],"content":"Transformer的结构 图1：Transformer 模型架构 (Vaswani et al., 2017) 下文依然以“我爱小猫”翻译成“I love little cats”的翻译为例子来解释。 Part 1: Encoder Input Embedding：首先，经过词嵌入，将每个词都转换成机器能理解的向量形式 “我爱小猫”这四个字，每一个字都被转换成一个512维的向量 词嵌入的向量包含词本身的原始语义信息 Positional Encoding：位置编码，通过一个公式，给每一个词都生成一个 512 维的位置编码的向量（公式详见 Section 3.5）。生成好的位置编码，直接通过相加的方式，叠加到每个词的词嵌入向量上。 $PE_{pos,2i}=sin(pos/10000^{2i/d_{model}})$ $PE_{pos,2i+1}=cos(pos/10000^{2i/d_{model}})$ 为什么位置编码如此复杂（还涉及到傅里叶计算）？不能直接给每个词一个位置编号0, 1, 2…？ 其实有的模型（比如 BERT）就是这样操作的 但这种编码是标量，信息量太少，模型无法理解，比如“第3个词和第10个词的关系有多远”，尤其是注意力机制是依靠向量计算的（比如点积），而单个数字在向量空间里几乎没法建立复杂的关系。 所以要把位置信息变成一个高维向量，其中包含的信息就可以很丰富了，比如：能让模型感受到不同位置之间的差异、捕捉到“某两个词之间的位置差”等等相对关系。 Multi-head Attention（多头自注意力）： 涉及到 $Q$、$K$、$V$ 矩阵。最后会使矩阵在包含原始的语义信息、上一步增加的位置编码之后，又增添了上下文信息。 单头自注意力机制的解释 QKV：Q（query）、K（key）、V（value） 首先，输入的是一个 4*512 的矩阵，代表“我爱小猫”。 然后，和三个矩阵，WQ、WK、WV 进行矩阵乘法。这三个矩阵都是 512*512 的尺寸，里面存储的是权重。 得到三个 4*512 的矩阵作为结果，分别是 Q、K、V 将 Q、K、V 三个矩阵代入 Attention 计算公式（Section 3.2.1，公式 1）： $$ \\text{Attention}(Q,K,V)=\\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V $$ 得到一个 4*512 的矩阵作为输出。 多头自注意力机制： WQ、WK、WV 这三个权重矩阵，大小为 512*64 于是得到的 Q、K、V 矩阵大小为 4*64 多头：即 WQ、WK、WV 一共有 8 组（512/64=8）。 而得到的 Q、K、V 矩阵也一共有8组。 将这 8 组 4*64 的矩阵拼接起来： 最后得到三个 4*512 大小的矩阵（和单头结果是一样大小的） 再经过一个线性层 得到输出，4*512 Add \u0026 Norm （残差连接 \u0026 归一化） 残差连接：把经过多头注意力机制处理前那一步的数据，直接相加到经过处理后的数据上——避免如果多头注意力处理得很差时，可能导致模型很差 归一化：使数值趋于稳定 Feed Forward（前馈神经网络） Add \u0026 Norm （又一次，残差连接 \u0026 归一化） 到这一步位置，就得到了编码器最终的输出结果。矩阵维度是没有变化的（这个例子里是 4*512）。 Part 2: Decoder ","date":"2025-10-24","objectID":"/251024-transformers/:2:1","tags":["Transformer","论文阅读","LLM","NLP"],"title":"Transformers 模型：《Attention is All You Need》论文笔记","uri":"/251024-transformers/"},{"categories":["Text Mining 课程笔记"],"content":"word2vec ","date":"2025-09-23","objectID":"/250923-tm-w3/:1:0","tags":["文本挖掘","课程笔记","word2vec","词嵌入"],"title":"Text Mining | 03: Vector Semantics | 语义向量与词嵌入","uri":"/250923-tm-w3/"},{"categories":["Text Mining 课程笔记"],"content":"用比喻来说明 给一大群词语安排座位，关系越密切的词语，互相离得就越近。你只有一本厚厚的书，记录了他们在不同场合的聊天记录。 训练过程 安排者拿到了一个巨大的宴会厅（也就是向量空间），开始随机地给每个词语安排一个座位。 然后，他开始阅读那本聊天记录，一次只看一句话，比如：“程序员用键盘写代码。”以“键盘”为中心，看到了旁边的次“程序员”和“代码”。 “拉近”操作（正）：把“键盘”和“程序员”在座位图里的位置，拉近一点点；把“键盘”和“代码”的座位也拉近一点点。 “推远”操作（负）：随机抓几个词语，比如“猫”、“蛋糕”，安排者心想：“蛋糕”和“键盘”应该关系不密切，因为没有在聊天记录里一起出现过，于是他把“蛋糕”的座位从“键盘”那里推远一点点。 不断重复以上操作 经过若干次调整后：程序员、键盘、代码……这些词聚集在了一起；医生、手术刀、病历……这些词也聚集在了一起；猫、猫粮、毛线球……这些词也聚集在了一起。 回到Word2Vec本身 每个词的座位===代表了每个单词的词嵌入向量（Word Embedding） 词嵌入向量：一个数字列表（也就是一个向量） 通常包含几百个小数 代表一个词在某个高维语义空间中的坐标 最终计算机通过比较不同词汇的词嵌入向量的相似度，来判断不同词汇在意义上有多接近。 聊天记录===代表了用来训练的海量文本 “拉近”和“推远”的操作===代表了训练算法（Skip-gram with negative sampling） 把复杂的分类问题，转换成了二元判断的问题。 ","date":"2025-09-23","objectID":"/250923-tm-w3/:1:1","tags":["文本挖掘","课程笔记","word2vec","词嵌入"],"title":"Text Mining | 03: Vector Semantics | 语义向量与词嵌入","uri":"/250923-tm-w3/"},{"categories":["Text Mining 课程笔记"],"content":"局限 Word2Vec无法处理多义词，比如，一个词语可能有两类不同的意义，于是就会在向量空间里处于两类词汇之间一个尴尬的中间位置。 ","date":"2025-09-23","objectID":"/250923-tm-w3/:1:2","tags":["文本挖掘","课程笔记","word2vec","词嵌入"],"title":"Text Mining | 03: Vector Semantics | 语义向量与词嵌入","uri":"/250923-tm-w3/"},{"categories":["Text Mining 课程笔记"],"content":"Preliminary knowledge ","date":"2025-09-23","objectID":"/250923-tm-w3/:2:0","tags":["文本挖掘","课程笔记","word2vec","词嵌入"],"title":"Text Mining | 03: Vector Semantics | 语义向量与词嵌入","uri":"/250923-tm-w3/"},{"categories":["Text Mining 课程笔记"],"content":"Distributional hypothesis （Linguistics） Words that occur in similar contexts tend to be similar 在相似上下文中出现的词语也趋向于相似。 ","date":"2025-09-23","objectID":"/250923-tm-w3/:2:1","tags":["文本挖掘","课程笔记","word2vec","词嵌入"],"title":"Text Mining | 03: Vector Semantics | 语义向量与词嵌入","uri":"/250923-tm-w3/"},{"categories":["Text Mining 课程笔记"],"content":"Vector Space Model（VSM） vector space：维数等于词汇表的大小。每一个单词都用向量来表示，基于单词之间的“同时出现次数”（co-occerrence）。 比如： digital 和 computer 同时出现较多，所以，表示 digital 的向量中，computer 那一位，数值就比较大；而比如 sugar 那一位，数值可能就非常小。 digital 向量可能长这样：[0, 0, …, (many zeors..), 999, 1789, 4,2,…] 其中 1789 那个就是 computer 和 digital 共同出现过这么多次。 在词向量中，每一个单词，都是一个 discrete dimension。 向量有许多的 0，是稀疏的（sparse），因为对每个单词来说，绝大多数的单词都不会更它大量地同时出现。 其他的语义表示方法：topics（topic modelling，Lecture 9 中讨论）；稀疏语义向量 — 词嵌入（word embeddings）— 密集（dense）、让相似的单词在向量空间中距离得更近。 ","date":"2025-09-23","objectID":"/250923-tm-w3/:2:2","tags":["文本挖掘","课程笔记","word2vec","词嵌入"],"title":"Text Mining | 03: Vector Semantics | 语义向量与词嵌入","uri":"/250923-tm-w3/"},{"categories":["Text Mining 课程笔记"],"content":"Feed-forward neural networks 多层网络，没有循环。输入单元、隐藏单元、输出单元。全连接。 二元分类：只有一个输出节点，y 代表其中 positive output 的概率 多类别分类：每一个类别都有一个输出节点（分几类，就有几个输出节点），y 代表某个类别的概率。所以，输出层代表了概率分布。比如：[0.9, 0.03, 0.02, 0.05] 就代表一个分 4 类的分类问题。 如何得到概率分布：通过 Softmax 函数 $\\text{softmax}(y_i)=\\frac{e^{y_i}}{\\sum_{j=1}^d e^{y_i}} \\text{ for } i\\leq i \\leq d$ d = dimensionality（即：分成几类） 输入一个向量，输出一个概率分布向量，让向量中每个值的总和为 1。 ","date":"2025-09-23","objectID":"/250923-tm-w3/:2:3","tags":["文本挖掘","课程笔记","word2vec","词嵌入"],"title":"Text Mining | 03: Vector Semantics | 语义向量与词嵌入","uri":"/250923-tm-w3/"},{"categories":["Text Mining 课程笔记"],"content":"Word embeddings 词嵌入是稠密向量，在连续的稠密向量空间中。（continuous dense vector space）向量空间相对来说是低维的。（10000 → 100） 向量中的数值（也代表向量在向量空间中的位置）是隐式的（latent）。人无法理解它们的具体含义。 符合分布假说：语义和句法上相似的单词，会距离更近。 （以下部分，是阅读 J\u0026M: Dan Jurafsky and James H. Martin, Speech and Language Processing (3rd ed), 2025 第五章的阅读笔记与思考。） Distributional hypothesis（分布假设）：语言学的概念，意思是，出现在类似语境中的词，往往意义相近。关键是：词在分布上的相似性与意义上的相似性之间的联系。 Embeddings（嵌入）：从文本中词的分布学习到的词的含义的向量化表示。 主要技术： Word Embedding 的生成：Word2vec （尤其是其中的 Skip-gram with negative sampling） 论文： Efficient Estimation of Word Representations in Vector Space: [PDF] Distributed Representations of Words and Phrases and their Compositionality: [PDF] ","date":"2025-09-23","objectID":"/250923-tm-w3/:3:0","tags":["文本挖掘","课程笔记","word2vec","词嵌入"],"title":"Text Mining | 03: Vector Semantics | 语义向量与词嵌入","uri":"/250923-tm-w3/"},{"categories":["Text Mining 课程笔记"],"content":"Vector Semantics 向量语义 思想： 用多维空间中的一个点来表示词的情感意义，比如用三个维度分别表示愉悦度（valence）、唤醒度（arousal）、支配度（dominance）。（1957 Osgood等人） 语言学中的分布思想：用词在语言中出现的上下文分布来定义其意义。一个词的含义由它出现在哪些环境、与哪些词相邻来决定。（1950 Joos、1954 Harris、1957 Firth） Vector semantics：向量语义的核心思想，是将词语表示为多维语义空间中的一个点（该空间通过不同方式从单词临近词的分布中推导而来）。用于表示单词的向量，成为嵌入（embeddings）。 Simple count-basaed embeddings 简单的基于计数的词嵌入 这是一种最简单的词向量模型，基于共现矩阵（co-occurrence matrix）。我们定义一种特别的共现矩阵：词-上下文矩阵（word-context matrix）。 行：词表中的每个目标词（target） 列：词表中每个上下文词（context） 单元格：目标词与上下文词在附近（nearby）共同出现的次数 向量空间（vector space）：由向量构成的集合，其特征由维数决定（dimension）。比如：[1, 0, 0, 0] 这里的维度就是4。 这种方法，每个向量的维度等于词汇表的维度（几万）。 Cosine for measuring similarity余弦相似度衡量词语相似性 NLP（自然语言处理）中，最常用的相似度度量是：余弦相似度——基于线性代数中的点积（dot product），也成为内积（inner product）： $v\\cdot w=\\sum_{i=1}^N v_iw_i=v_1w_1+v_2w_2+\\cdots+v_iw_i$ 点积越大，通常说明：两个向量在相同维度上都有较大值 如果点积为0，则说明：两个向量垂直；两个向量非常不相似 点积的缺陷 \u0026 点积归一化 点积偏向长向量。向量的长度定义为： $|v|=\\sqrt{\\sum_{i=1}^N v_i^2}$ 如果一个向量比较长，则意味着它的每个维度都有比较大的值，而它与其他向量计算出来的点积也会更大。 前面说的基于count的向量语义方法，越频繁出现的词语，它的向量长度越大，因此，与任何词的点积都会比较大。但它并不应该与任何词都有较高的相似度——我们希望相似度的计算不受词语的出现频率的影响。 把原始的点积除以向量的长度，实现点积归一化（normalized dot product）： $\\cos(\\theta)=\\frac{v\\cdot w}{|v||w|}$。归一化的点积结果等于两个向量之间夹角的余弦值。两个向量v和w之间的余弦相似度的计算如下： $$ \\text{cosine}(v,w)=\\frac{v\\cdot w}{|v|~|w|}=\\frac{\\sum_{i=1}^Nv_iw_i}{\\sqrt{\\sum_{i=1}^Nv_i^2}\\sqrt{\\sum_{i=1}^Nw_i^2}} $$ 向量完全相同：余弦相似度=1 向量完全不相关（正交）：余弦相似度=0 向量完全相反：-1 （但这里不会出现，因为词计数非负） ","date":"2025-09-23","objectID":"/250923-tm-w3/:3:1","tags":["文本挖掘","课程笔记","word2vec","词嵌入"],"title":"Text Mining | 03: Vector Semantics | 语义向量与词嵌入","uri":"/250923-tm-w3/"},{"categories":["Text Mining 课程笔记"],"content":"Word2vec 前面介绍的基于count的方法，词语被表示为稀疏（sparse）、且很长的向量，向量的维度等于词汇表的大小（有多少个不同的词，就有多少个维度）。所以这些向量维度巨大、大部分都是0（非常稀疏）。下面介绍的更强的方法：词嵌入（embeddings）。 词嵌入 Embeddings 词嵌入是短小、密集的向量。 维度通常为50~1000 每个维度并没有清晰的含义 每个值都是实数，也可能为负数 NLP的任务中，比起上万的维度，使用词嵌入把词语表示为300维的向量，效果更好： 我们的分类器（classifier）需要学习的权重少了很多 更小的参数空间对于泛化（generalization）更有帮助，可以避免过拟合。 更好地找到同义词（synonyms） 接下来介绍一种计算词嵌入的方法：skip-gram with negative sampling（SGNS）。 Skip-gram with negative sampling (SGNS) 参考链接： 让电脑听懂人话：理解 NLP 重要技术 Word2Vec 的 Skip-Gram 模型 Word2Vec Tutorial - The Skip-Gram Model Word2Vec Tutorial Part 2 - Negative Sampling Word2vec模型中，主要有CBOW与Skip-gram两种模型。CBOW是给定上下文来预测输入的字词，而Skip-gram是给定输入字词后，来预测上下文。这里讨论Skip-gram模型。 我们的任务是：训练一个神经网络，在给定一段句子中的一个字词后，可以告诉我们其他字词出现在它附近的概率。 举例说明： The quick brown fox jumps over the lazy dog. 把 window size 设为 2，图中蓝色部分的字词为输入的词 如 “the” 这个词可以产生（the，quick）和（the，brown）两对训练样本 而 “quick” 这个词可以产生（quick，the）、（quick，brown）和（quick，fox）这三对训练样本 根据输入的语料，我们训练的神经网络就会开始统计每一组成对出现的字词出现的次数。 比如，可能（cake，sweet）出现的次数会比（cake，AI）出现的次数多——意味着 cake 和 sweet 的含义更接近，sweet 出现在 cake 附近的概率，要高于 AI 出现在 cake 附近的概率。 在神经网络中，输入层是一个用 one-hot 编码表示的向量（1 x 10000），而输出层也是一个相同维度、用 ont-hot 编码表示的向量，会转换成概率分布，用来表示每个字词出现在输入词附近的概率。 隐藏层（hidden layer）中，原论文使用了 300 个 features，让神经网络变成了一个 10000 行 x 300 列的矩阵。神经网络的目标，就是训练出隐藏层中，这个 10000 x 300 尺寸的矩阵中每一个值。 举个例子：假设输入向量只用 5 维来表示一个词，隐藏层中只设定了 3 个features 矩阵相乘的结果，其实就相当于隐藏层的权重矩阵中，对应 one-hot 编码中为 1 的那个维度的那一行的数值的向量——这个例子里这个 1 x 3 维度的向量，就成为输入的字词的词向量（word vector）。 回到一开始那个 10000 x 300 的矩阵，我们为每个输入词都会得到一个 1 x 300 的向量——用来表示输入的那个词。 我们把一个 1 x 300 的某个输入词的向量，乘上另一个词的权重向量，得到的数字再进行 softmax 函数处理，这个值就会变成一个 0 ~ 1 之间的概率数值，代表第二个词出现在我们输入的那个词附近的概率。 提高模型训练效率的方案： 将常见的单词组合（word pairs）或是词组（phrase）当做是单个字来处理。 对高频率出现的字词进行抽样，来减少训练样本的数目。 比如 “the” 出现频率非常高，会产生大量（the，XXX）这这种训练样本，样本数目可能远超我们学习 “the” 这个词向量所需要的训练样本数；而且，对于学习 “XXX” 这个词的语义，帮助可能也不大。 解决方案：抽样（subsampling） 对于优化目标，使用负采样（negative sampling）的方式，让每个训练模型，可以只更新一小部分的权重，来降低运算的负担。 负采样（negative sampling）：对于每个训练样本，只更新一部分的权重，而非整个神经网络的权重都被更新。 Skip-gram with negative sampling 学习到的向量是静态词嵌入（static embeddings），每个词只有一个固定的向量，不随上下文变化——这与后来的 BERT 不同（BERT 是动态词嵌入）。 word2vec 的革命性思想：不去数在某个词附近另个词出现的次数，而是训练一个分类器，去看另个词在某个词附近出现的概率有多大？但我们不关心分类结果，而是把分类器学习到的权重当做词向量。 而训练标签来自文本本身，不需要人工标注，这种方式叫做自监督学习（self-supervision）。大量现代 NPL 模型例如 BERT 也依赖这个思想。 The model Skip-Gram 模型的目标就是学习用来表示词语的词向量。 思想：构建一个简单的神经网络 但最后我们并不会用这个网络来完成某个任务，我们真正想要的是这个网络的隐藏层的权重 —— 那就是我们想要的词向量。 一个输入层 输入的是：ont-hot encoded 向量，长度等于整个词汇表的大小 （比如 10000）。 例如我们输入 “cat” 这个词，向量中除了代表 “cat” 的那个位置是 1以外，其他所有位置都是 0。 一个隐藏层 节点数决定了我们最终学习到的词向量的维度（比如300） 输入层到隐藏层之间有一个巨大的权重矩阵 W1 矩阵的行数 = 词汇表的大小（比如10000） 矩阵的列数 = 词嵌入的维度（比如300） 所以这里的权重矩阵是 10000 x 300 大小的，而因为输入是 one-hot 编码，所以它乘以这个权重矩阵的结果，就等同于直接从矩阵中取出对应 one-hot 为 1 的那一行。 所以，权重矩阵 W1 的每一行，就是每个词的词向量。 一个输出层 输出的是：长度和输入向量一样的巨大的向量，长度也等于词汇表大小（10000） 向量中每个值，代表每个单词是输入词的上下文的概率。 使用 Softmax 函数，来确保这些值的总和等于 1，形成概率分布。 举例说明正向传播的过程： 将输入词 “cat” 的 one-hot 向量（比如第 10 位为 1）输入给网络 这个 one-hot 向量乘以权重矩阵 W1（10000 x 300）。 相当于：从 W1 中选择了第 10 行。这一行就是 “cat” 这个词的词嵌入向量（embedding） 这个 300 维的向量（h）被传递给输出层 为了得到输出层，用 h 乘以第二个权重矩阵 W2（300 x 10000） 这个 1 x 10000 的结果向量，就是每个词的“概率得分”，我们对它应用 Softmax 函数，将其转换为概率分布 最后结果是：网络输出一个 1 x 10000 的概率分布向量。向量中每个值都代表了词汇表中对应单词是 “cat” 这个输入词的上下文的概率。 训练过程 训练的目标：调整权重矩阵 W1 和 W2，使网络的预测结果更准确。 用反向传播和梯度下降。 用举例来解释训练的过程： 当网络看到训练样本 （cats, animal）时： 先进行正向传播，得到一个预测的概率分布 计算这个预测与真实目标（即：一个在 “animal” 位置为 1，其他位置为 0 的 ont-hot 向量）之间的误差 这个误差会被反向传播，用来更新权重矩阵 W1 和 W2 让输入 “cats” 时，输出里 “animal” 的概率变高。 “dogs” 同理。久而久之，它们的词向量（也就是 W1 中的对应两行）就会变得越来越相似。 Negative sampling 负采样 之前我们的任务是让模型预测上下文词，我们把它变为：判断一个词是“好”是“坏”。 举例说明：模型会接收一对词，比如（cat，animal），然后输出一个概率，用来判断它们是不是一个“中心词-上下文词”的组合。如果是，我们希望网络输出的概率接近 1；如果不是，我们希望网络的输出接近 0。 正例和负例的采样 为了训练这个新的网络，我们从训练文本中提取词对（word pair），并给它们打上标签 1 或 0（1 代表正例，即真实的上下文词对；0 代表负例）。 正例：生成方式和此前一样。使用一个滑动窗口扫过文本，所有在窗口内形成的“中心词-上下文词”对都是我们的正例。 负例：对于每一个正例，我们从词典中随机抽取 k 个词，来生成 k 个负例。 举例解释负例的生成： 例如：对于正例（cat，animal），我们可能随机抽取到 “dog”, “information”, “kitchen” 等作为负样本。然后将会生成以下这些带标签的训练样本： cat - animal - 1 cat - dog - 0 cat - information - 0 cat - kitchen - 0 采样的方法：并非完全地随机采样，不然像 “a”、“the”、“of” 这类高频词被抽中概率会非常高，而它们并不能提供很有价值的信息。我们希望抽到更有意义的词。因此，用一种特殊的抽样方法。 抽中某个词 w 的概率 P","date":"2025-09-23","objectID":"/250923-tm-w3/:3:2","tags":["文本挖掘","课程笔记","word2vec","词嵌入"],"title":"Text Mining | 03: Vector Semantics | 语义向量与词嵌入","uri":"/250923-tm-w3/"},{"categories":["Text Mining 课程笔记"],"content":"Raw text → 通过 OCR（Optical Character Recognition）：将原始文本，比如手写文本，经过扫描，生成数字文件（digitized documents） 也有一些本来就是数字形式的原始文本，比如：HTML、text 文件、PDF、MS Word 所有的文本都需要一些清理（clean-up） 图片、表格等；设计的格式、排版；声明、版权说明；headers、footers；分列、页面空白； 以及：OCR errors；character encoding errors character encoding：计算机如何将文本呈现成人类理解的方式 semi-structured text：markup（HTML、XML、json） markup：文本文件中的元数据（meta-information），和文本内容能区分出来。 ","date":"2025-09-16","objectID":"/250916-tm-w2/:0:0","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 02: Preprocessing | 预处理","uri":"/250916-tm-w2/"},{"categories":["Text Mining 课程笔记"],"content":"Character encoding ","date":"2025-09-16","objectID":"/250916-tm-w2/:1:0","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 02: Preprocessing | 预处理","uri":"/250916-tm-w2/"},{"categories":["Text Mining 课程笔记"],"content":"ASCII 二级制序列 → 字符（character） ASCII (American Standard Code for Information Interchange) 是 7 位的编码方式，基于英文字母。 ","date":"2025-09-16","objectID":"/250916-tm-w2/:1:1","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 02: Preprocessing | 预处理","uri":"/250916-tm-w2/"},{"categories":["Text Mining 课程笔记"],"content":"Unicode Unicode 是所有语言系统、全球通用（universal）的标准。 不依赖于任何平台、软件、供应商（vendor）。 通过软件（app、editor、web browser）中的实现（例如 UTF-8）来将编码解析成字符。而软件来决定实际的呈现（大小、形状、字体、风格）。 # Read and write UTF-8 in Python 3 with open(filename, 'r', encoding='utf-8') as raw: text = raw.read() with open(filename, 'w', encoding='utf-8') as clean: clean.write(text) https://docs.python.org/3/library/functions.html#open ","date":"2025-09-16","objectID":"/250916-tm-w2/:1:2","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 02: Preprocessing | 预处理","uri":"/250916-tm-w2/"},{"categories":["Text Mining 课程笔记"],"content":"关于实际中的 Data cleaning Digitalizaiton, data conversion \u0026 cleaning：是第一步。很有必要、耗时间、会出错、常常复杂。 数据收集与清理的过程一般比较漫长。 ","date":"2025-09-16","objectID":"/250916-tm-w2/:1:3","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 02: Preprocessing | 预处理","uri":"/250916-tm-w2/"},{"categories":["Text Mining 课程笔记"],"content":"Regular expressions 正则表达式：removing noise, privacy sensitive information, markup。 用来捕捉、计数一个文件或字符串里面符合的模式；或者提取符合的模式。 Spacy ","date":"2025-09-16","objectID":"/250916-tm-w2/:2:0","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 02: Preprocessing | 预处理","uri":"/250916-tm-w2/"},{"categories":["Text Mining 课程笔记"],"content":"Tokenization \u0026 sentence splitting ","date":"2025-09-16","objectID":"/250916-tm-w2/:3:0","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 02: Preprocessing | 预处理","uri":"/250916-tm-w2/"},{"categories":["Text Mining 课程笔记"],"content":"定义 Token：word or punctuation（单词或标点符号） Term：用作特征的 token，一般是 normalized form（小写） Token count：包含重复 Vocabulary size |V|：不重复的 term 数。把单词用作 feature 时即为 feature size。 Example This issue is not just local — it is a global problem. Token count: 13 Unique terms: 10（不包括标点和重复的单词） Vocabulary size |V|: 10（同上） ","date":"2025-09-16","objectID":"/250916-tm-w2/:3:1","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 02: Preprocessing | 预处理","uri":"/250916-tm-w2/"},{"categories":["Text Mining 课程笔记"],"content":"Tokenization 将文本分割成 tokens。 去掉标点 按空格分割文本 ","date":"2025-09-16","objectID":"/250916-tm-w2/:3:2","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 02: Preprocessing | 预处理","uri":"/250916-tm-w2/"},{"categories":["Text Mining 课程笔记"],"content":"Subword tokenizers 不再把 tokens 定义成按空格分开的单词或是中文的汉字，而是根据数据，能自动判断哪些是 tokens。 尤其适合处理不认识的新词 回顾 Heap’s Law：在某集合上训练模型时，新添加的词会越来越多 解决方案： sets of tokens → subwords 把低频词分成词汇表里存在的 tokens。 subwords 本身可能并不是单词 proportionate → prop portion ate Byte-pair encoding（BPE） WordPiece/SentenePiece Byte-pair encoding（BPE） begin with a vocabulary 检查要训练的 corpus，选择相邻着出现的频率最高的两个符号 把这两个符号连起来（例如：t、h → th）添加进 vocabulary 把所有相连着出现的这两个符号，都替换成连起来的新字符串（例如‘th‘） 重复第二步，创造新的越来越长的字符串 直到完成了 k 次，创建了 k 个新的 tokens（k 是参数） 最后，vocabulary 里：原有的字符 + k 个新 tokens WordPiece/SentencePiece 基于 BPE，高效的实现。 vocabulary size 固定，优化 tokens 的选择。 高频出现的词语成为 token，低频词被分割成 subwords（例如：Gorlaeus → Go + ##rl + ##ae + ##us’）。 ","date":"2025-09-16","objectID":"/250916-tm-w2/:3:3","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 02: Preprocessing | 预处理","uri":"/250916-tm-w2/"},{"categories":["Text Mining 课程笔记"],"content":"Lemmatization \u0026 Stemming Lemma（词形） Stem（词干） 减少 word vector space 中的 features 总数（就是减少单词数量的意思） 更好地泛化，尤其是小数据集 ","date":"2025-09-16","objectID":"/250916-tm-w2/:4:0","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 02: Preprocessing | 预处理","uri":"/250916-tm-w2/"},{"categories":["Text Mining 课程笔记"],"content":"Lemma 词形 定义：dictionary form of a word 举例： 动词的原形（infinitive of verbs） think ← thinks, thinking, thought 名词的单数形式（singular form of nouns） mouse ← mice computer ← computers Lemmatization 需要词典。 ","date":"2025-09-16","objectID":"/250916-tm-w2/:4:1","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 02: Preprocessing | 预处理","uri":"/250916-tm-w2/"},{"categories":["Text Mining 课程笔记"],"content":"Stem 词干 定义：the portion of a word that is common to a group of words with the same prefix (base/root) 相关的单词会有相同的词干。 举例： computer, computing, computers, compute → ‘comput’ 词干自己不一定是一个存在的单词。 在长得不一样的单词中，词干总是那个一直不变的形式。就比如前一个例子里的那个“comput”。 我们一般总是更倾向于 lemma 而不是 stem。 非常小的集合，或是资源很少的语言，stemming 的效果才会更好。 Note： 因为 lemmazation 基于字典，把词还原成原形，结果是真实的单词，能看懂含义。还擅长不规则变形的词形还原，比如 better → good。 Stemming 是基于规则，比较生硬，得到的结果往往都不是真实的单词，人看不懂含义。不擅长不规则变形的单词，better 可能没法还原成 good。 但是，低资源的语言（如某些土著语言、濒危语言）没有字典、没有标注数据。 用一个例子来总结 Original text: Such an analysis can reveal features that are not easily visible from the variations in the individual genes and can lead to a picture of expression that is more biologically transparent and accessible to interpretation Lematizer: Such an analysis can reveal feature that be not easily visible from the variation in the individual gene and can lead to a picture of expression that be more biologically transparent and accessible to interpretation Stemmer: Such an analysi can reveal featur that ar not easili visibl from the variat in the individu gene and can lead to a pictur of express that is more biolog transpar and access to interpret Subword tokenizer: Such an analysis can reveal features that are not easily visible from the variations in the individual genes and can lead to a picture of expression that is more biological ##ly transparent and accessible to interpretation ","date":"2025-09-16","objectID":"/250916-tm-w2/:4:2","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 02: Preprocessing | 预处理","uri":"/250916-tm-w2/"},{"categories":["Text Mining 课程笔记"],"content":"Edit distance 衡量字符串相似度时，如何处理拼写错误、同一个词不同的拼写方式呢？ 最小编辑距离（minimum edit distance）：把一个字符串变成另一个字符串所需要的最小编辑操作次数 cost of 1： insertion cat → coat deletion cat → at substitution（替换） cat → cut free：copying charaters 把一个单词变成另一个单词，有很多路径，但总有一条路最简单（cheapest） 比如：sheep → ship copy ‘sh’, delete ‘e’, substitute ‘e’ by ‘p’ (edit distance = 2) copy ‘sh’, delete ‘e’, delete ‘e’, insert’i’, copy ‘p’ (edit distance = 3) 计算最小编辑距离：Dynamic programming。是一类算法，用一种基于表格的方法来解决问题。 ","date":"2025-09-16","objectID":"/250916-tm-w2/:5:0","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 02: Preprocessing | 预处理","uri":"/250916-tm-w2/"},{"categories":["Text Mining 课程笔记"],"content":"Levenshtein distance algorithm 画一个表格。左边第一列是原本的单词，上边第一行是目标单词。 往右走：Insert 往下走：Delete 走对角线：Substitute or Copy 选择最小消耗的行动 直到走到右下角 消耗依然是：Insert(1), Delete(1), Substitute(1), Copy(0) 图片来源：Leiden University Text Mining Course Slides ","date":"2025-09-16","objectID":"/250916-tm-w2/:5:1","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 02: Preprocessing | 预处理","uri":"/250916-tm-w2/"},{"categories":["Text Mining 课程笔记"],"content":"Text mining: Automatic extraction of knowledge from text 从文本中自动地提取知识（信息） Text: unstructured; Knowledge: structured Big text data —(Text retrievl)→ Small relevant data —(Text mining)→ Knowledge → Many applications Challenges of text data: unstructured noisy (source, content, labels) language is infinite Heap’s Law: 语料库（corpus）很小的时候，新词的数目增长非常快，而且会无限地继续增长。 language is ambiguous ","date":"2025-09-07","objectID":"/250907-tm-w1/:0:0","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 01: Introduction","uri":"/250907-tm-w1/"},{"categories":["Text Mining 课程笔记"],"content":"Text mining pipeline Information Retrival (IR) Pre-processing NLP Types of text processing tasks ","date":"2025-09-07","objectID":"/250907-tm-w1/:1:0","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 01: Introduction","uri":"/250907-tm-w1/"},{"categories":["Text Mining 课程笔记"],"content":"Text classification/clustering 为每个 document 分类，document 可以是任何文本类型（报纸文章、推文、email、文字信息、一个句子。类别可以是任何的 label（主题、相关度、作者、情感等）。 ","date":"2025-09-07","objectID":"/250907-tm-w1/:2:0","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 01: Introduction","uri":"/250907-tm-w1/"},{"categories":["Text Mining 课程笔记"],"content":"Sequence labelling (= named entity recognition) 为文本中的每个词分类。 例如：在文本中识别人名、地名（NER）。 ","date":"2025-09-07","objectID":"/250907-tm-w1/:3:0","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 01: Introduction","uri":"/250907-tm-w1/"},{"categories":["Text Mining 课程笔记"],"content":"Text-to-text generation 输入文本，输出文本。 总结（summarization）：sequence-to-sequence 翻译 Text mining models ML 的范式： supervised learning 训练 featured-based 模型 lightweight，explainable transfer learning 根据任务，fine-tune 预训练模型（如 BERT） 有足够的 labelled data 的 TM 任务的最佳选择 in-context learning 给 LLM prompt 和一些例子作为引导 没有 labelled data 的 TM 任务的一个选择 监督学习和迁移学习都需要大量的 labelled data。 ","date":"2025-09-07","objectID":"/250907-tm-w1/:4:0","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 01: Introduction","uri":"/250907-tm-w1/"},{"categories":["Text Mining 课程笔记"],"content":"文本作为分类对象 \u0026 文本作为序列（sequence） 把文本作为整体进行分类时，文字顺序（word order）没有那么重要，而把文本作为序列时，文字顺序比较重要。 文本作为分类对象：垃圾邮件处理、新闻分类 关注整体内容 文本作为序列：给句子里每个词打标签（人名、地名、动词等） 关注上下文结构、词序 ","date":"2025-09-07","objectID":"/250907-tm-w1/:5:0","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 01: Introduction","uri":"/250907-tm-w1/"},{"categories":["Text Mining 课程笔记"],"content":"文本分类 基于特征的文本分类方法把文本表示为“bag of words”。 bag-of-words 模型里，集合里的每个单词都是一个特征（feature）。 传统的 bag-of-words 模型中，词序、标点符号（punctuation）、句子和段落的分界线都不重要。 词袋模型的缺点： 维度爆炸 - 维度太高了，假如词典里有 10000 个词，每个单词的向量长度就都是 10000。 稀疏（sparsity） - 词向量的大部分位置都是 0，浪费资源 ","date":"2025-09-07","objectID":"/250907-tm-w1/:6:0","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 01: Introduction","uri":"/250907-tm-w1/"},{"categories":["Text Mining 课程笔记"],"content":"Zipf’s Law Frequency 与 rank 成反比：一个词出现的频率，与它在频率表里的排名成反比。也就是说，在频率表里越靠前的词，越频繁出现；很靠后的词，几乎不出现。 Lont-tail distribution：头部（head）是极少数出现的词，但出现次数最多；长长的尾巴（tail）是绝大多数的词，但它们包含各种含义却出现次数非常少。 所以：要去掉“头部”这些词，也就是停用词（stop word）；也要小心处理尾部，因为数据太少。 ","date":"2025-09-07","objectID":"/250907-tm-w1/:6:1","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 01: Introduction","uri":"/250907-tm-w1/"},{"categories":["Text Mining 课程笔记"],"content":"文本的稠密表示 word embeddings（词嵌入）：低维、稠密、隐式的（latent）、语义向量。 比如：word2vec、GloVe、BERT 在 Lecture 3 讨论词嵌入。 ","date":"2025-09-07","objectID":"/250907-tm-w1/:6:2","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 01: Introduction","uri":"/250907-tm-w1/"},{"categories":["Text Mining 课程笔记"],"content":"文本序列 从文本中提取知识，就要关注词序、标点符号、大小写。 识别人名、时间、电影名字等等。 Transformer models sequence-to-sequence 任务：输入文本，输出文本。 比如：翻译。 Transformer 结构有两个模块：Encoders，Decoders。分别负责处理输入文本和生成输出文本。 在 Lecture 6 讨论 Transformer。 ","date":"2025-09-07","objectID":"/250907-tm-w1/:7:0","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 01: Introduction","uri":"/250907-tm-w1/"},{"categories":["Text Mining 课程笔记"],"content":"BERT Bidirectional Encoder Representations from Transformers (BERT)：只有 encoder；输入文本，输出 embeddings。 适用于：分类（例如 sentiment）、序列标注（sequence labelling，例如 NER）。 ","date":"2025-09-07","objectID":"/250907-tm-w1/:7:1","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 01: Introduction","uri":"/250907-tm-w1/"},{"categories":["Text Mining 课程笔记"],"content":"GPT Generative Pre-trained Transforer (GPT)：只有 decoder；输入 prompt，输出文本。 decoder 模型巨大的时候，比如 GPT-4，也可以做 sequence-to-sequence 任务、甚至是分类任务，只要在 prompt 里很好地描述。 ","date":"2025-09-07","objectID":"/250907-tm-w1/:7:2","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 01: Introduction","uri":"/250907-tm-w1/"},{"categories":["Text Mining 课程笔记"],"content":"Encoder-decoder models T5：总结、翻译 在 Lecture 6、7 讨论 LLMs。 ","date":"2025-09-07","objectID":"/250907-tm-w1/:7:3","tags":["文本挖掘","课程笔记"],"title":"Text Mining | 01: Introduction","uri":"/250907-tm-w1/"}]